---
title: "Class08_Mini_Project"
author: "Ivan Henry Kish(PID:A17262923)"
format: pdf
toc: true 
---

## Background

In today's class we will be using all the R techniques for data analysis that we have learned so far ,including the machine learning methods of clustering and PCA, to anlayze real breast cancer biopsy data.

## Data Import

The data is in CSV format:
```{r}
fna.data <- "WisconsinCancer.csv"
wisc.df <- read.csv(fna.data, row.names=1)
```
Let's look at the data
```{r}
head(wisc.df,3)
```


>Q1. How many observations are in this data set?

```{r}
nrow(wisc.df)
```

>Q2. How many of the observations have a malignant diagnosis?

```{r}
table(wisc.df$diagnosis)
```

Here we have 357 observations that are benign and 212 observations that are malignant.

>Q3. How many variables/features in the data are suffixed with _mean?

```{r}
sum(grepl(pattern = "_mean", x = colnames(wisc.df)))
```

We now need to remove the `diagnosis` column before we do any further analysis of this data set, we don't want to pass the is to PCA. WE will save it as a separate vector that we can use later to compare our fundings to those of experts.

```{r}
wisc.data<- wisc.df[,-1]
diagnosis <- wisc.df$diagnosis
```

## Principal Component Analysis (PCA)

The main function in base R is called `pcomp()` we will use the optional argument `scale=T` here as the data columns/features/dimensions are on very different scales.


The main function in base R is called `prcomp()`

```{r}
wisc.pr <- prcomp(wisc.data, scale=T)
```

```{r}
attributes(wisc.pr)
```

```{r}
library(ggplot2)
ggplot(wisc.pr$x) + aes(PC1,PC2,col=diagnosis) + geom_point()

```

>Q4. From your results, what proportion of the original variance is captured by the first principal component (PC1)?

```{r}
summary(wisc.pr)

```
The proportion of variance captured by PC1 is 44.27% of the variance.

>Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

You need three principal components to describe at least 70% of the variance.

>Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

You need seven principal components to capture 90% of the data. 

##Interpreting PCA results

>Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

```{r}
biplot(wisc.pr)
```
This plot is very hard to read there are too many data points and labels for anyone to tell what they are looking at.

## Making it clear
We made it more clear earlier by using ggplot to plot PC1 vs PC2 to determine the trends seen in patients who had malignant and begnin masses.

>Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
ggplot(wisc.pr$x) +
  aes(PC1, PC3, col=diagnosis) +
  geom_point()
```

The two plots are very similar in terms of the separation between malignant and benign observations but lie on a more linear scale.

##Variance Explained

A scree plot shows how much variance each PC captures. We typically look for an “elbow” — a point where adding more PCs gives diminishing returns. This can help us decide how many PCs to consider for further analysis.

```{r}
# Calculate variance of each component
pr.var <-wisc.pr$sdev^2
head(pr.var)
```

We will now calculate the variance explained by each principal component divided  by the total variance explained of all principal components.


```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(c(1,pve), xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```



##Communicating PCA results

In this section we will check your understanding of the PCA results, in particular the “loadings” and “variance explained”.

The loading vector `wisc.pr$rotation` tells us which original measurements contribute most to each PC.

>Q9. For the first principal component, what is the component of the loading vector (i.e. `wisc.pr$rotation[,1]`) for the feature `concave.points_mean`? This tells us how much this original feature contributes to the first PC. Are there any features with larger contributions than this one?

```{r}
wisc.pr$rotation[,1]

```
Concave.points_mean is the biggest driver of variance in each principal component since there is no other component with a larger absolute value of a loading vector.


## Hierarchical clustering

The goal of this section is to do hierarchical clustering of the original data to see if there is any obvious grouping into malignant and benign clusters.
```{r}
data.scaled <- scale(wisc.data)
data.dist <- dist(data.scaled)
wisc.hclust <- hclust(data.dist)
```


## Results of hierarchical clustering

Let’s use the hierarchical clustering model we just created to determine a height (or distance between clusters) where a certain number of clusters exists.

```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```
>Q10. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

We have 4 clusters at height 19.

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, h=19)
```

>Q12.Which method gives your favorite results for the same data.dist dataset? Explain your reasoning

Ward.D2 gave the most visually understandable results.


##Combining methods

```{r}
pc.dist <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(pc.dist, method="ward.D2")
```

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table(grps, diagnosis)
```

```{r}
ggplot(wisc.pr$x) +
  aes(PC1, PC2) +
  geom_point(col=grps)
```

>Q13. How well does the newly created hclust model with two clusters separate out the two “M” and “B” diagnoses?

```{r}
# Compare to actual diagnoses
table(grps, diagnosis)
```
>Q14. How well do the hierarchical clustering models you created in the previous sections (i.e. without first doing PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.hclust.clusters and wisc.pr.hclust.clusters) with the vector containing the actual diagnoses.

```{r}
table(wisc.hclust.clusters, diagnosis)
```

## Prediction

We will use the predict() function that will take our PCA model from before and new cancer cell data and project that data onto our PCA space

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```
```{r}
plot(wisc.pr$x[,1:2], col=grps)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```
>Q16. Which of these new patients should we prioritize for follow up based on your results?

Based on these results and compared to our PCA plot from earlier patient 2 more represents he malignant group meaning that they are a higher priority.